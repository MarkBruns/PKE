# 2025-09-16

With the mdBook P.A.R.A. project structure largely in place, the second week is dedicated to installing, configuring, and establishing a working ***relationship*** with the [core open source agentic assistant, Cline](https://docs.cline.bot/getting-started/what-is-cline), but we will be evaluating/following the progress of the [multi-branch version control client, GitButler](https://github.com/gitbutlerapp/gitbutler) powered by Tauri/Rust/Svelte. Both Cline and GitButler are probably best equipped by their designers to be used with [Claude Code](https://www.anthropic.com/claude-code) or [Antropic's Claude Sonnet 4 LLM model](https://openrouter.ai/anthropic/claude-sonnet-4).

The primary tool for this project, the [Cline VS Code extension](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev), will be installed directly from inside VS Code Extensions marketplace. Configuration is the next critical step. An API key from [OpenRouter](https://openrouter.ai/docs/overview/principles) will be generated and added to the Cline settings. OpenRouter serves as a powerful API gateway, providing access to a multitude of LLMs from various providers like Anthropic, Google, and OpenAI. Initial experiments will be conducted using different models ... first, we might look into using the powerful [Claude Sonnet 4 LLM model](https://openrouter.ai/anthropic/claude-sonnet-4), especially for complex tasks, and then maybe various free-tier models, to gain an intuitive understanding of the cost-performance trade-offs inherent in different models.

However, the intention is not to play it safe with expensive recommended best version, but instead try different things with Cline, like using models like the economical [Grok Code Fast](https://openrouter.ai/x-ai/grok-code-fast-1) or even less expensive [Qwen Code](https://openrouter.ai/qwen) on [OpenRouter](https://openrouter.ai/docs/overview/principles) or the much cheaper option of [downloading LLMs like Qwen, Mistral, Gemma, or gpt-oss in LM Studio](https://lmstudio.ai/models) and running them [locally on Windows, MacOS, Linux](https://lmstudio.ai/docs/app/system-requirements) with [LMStudio](https://lmstudio.ai/docs/app/basics).

ULTIMATELY ... this whole PKM thing ... is heading the direction of something like [LangChain, LangGraph, LangFlow, LangSmith](https://medium.com/@anshuman4luv/langchain-vs-langgraph-vs-langflow-vs-langsmith-a-detailed-comparison-74bc0d7ddaa9) or a completely different product like [Manus](https://giancarlomori.substack.com/p/manus-ai-the-rise-of-the-general) from a separate company and is not part of the LangChain ecosystem. Whereas, LangChain ecosystem tools require a developer to build and orchestrate the workflow, [Manus](https://giancarlomori.substack.com/p/manus-ai-the-rise-of-the-general) is architecturally designed to handle the entire process autonomously based on a user's objective.