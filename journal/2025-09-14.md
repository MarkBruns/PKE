# 2025-09-14

Before automating the deployment process, it is **crucial** to understand the underlying mechanics that enable the automation and something about what that automation can mean in a larger, more systemic worldview ... even [a relatively basic 101-level blog post, talking about software engineering (SWE) coding agents that run independently in the background, completing assigned tasks like a junior peer developer, spinning up a fully customized changes in one's development environment, powered by GitHub Actions](https://github.blog/ai-and-ml/github-copilot/github-copilot-coding-agent-101-getting-started-with-agentic-workflows-on-github/) illustrates how important this topic is.

Personal Knowledge Management (PKM) systems will transform how individuals handle information overload by automating the grunt work across [diverse, rapidly evolving knowledge areas](https://github.com/AncientGuy/PKM/tree/main/src/2.Areas). Consider a biotech researcher working on health-fitness products and diagnostic devices, simultaneously tracking CRISPR developments, FDA regulations, and competitor patents—each requiring specialized AI agents with domain-specific AGENTS.md configurations. Each domain—whether financial analysis, scientific research, or engineering design—will deploy custom AGENTS.md configuration files alongside standardized protocol servers: [MCP (Model Context Protocol)](https://modelcontextprotocol.io/docs/getting-started/intro) for tool/data access and [A2A (Agent-to-Agent)](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/) for multi-agent coordination. 

In practice, a typical workflow might involve an A2A-coordinated agent team where a research agent uses MCP to query [academic databases](https://www.semanticscholar.org/product/api), while a data processing agent accesses computational tools through its own MCP connection. This dual-protocol architecture replaces what historically required . The emergence of the MCP and A2A protocols as standards democratizes capabilities once exclusive to Fortune 500 companies with dedicated data engineering departments with large multi-level 50+ person engineering teams managing ETL pipelines for disparate data sources from all kinds of things ranging from [Siemens NX CAD files](https://www.plm.automation.siemens.com/global/en/products/nx/) to [FlightAware API feeds](https://flightaware.com/commercial/flightxml/) to [ESRI ArcGIS spatial datasets](https://www.esri.com/en-us/arcgis/products/arcgis-platform/overview) to [SAP ERP systems](https://www.sap.com/products/erp.html) or [Boeing's CATIA models](https://www.3ds.com/products-services/catia/) or [NASA's Earth observation data](https://earthdata.nasa.gov/). These gigantic oceans of corporate data required many multi-millions of recurrent middleware investments and upgrades ... but we have reached the point where individuals can now access similar power through properly configured agent networks. Individual researchers or very small teams can now orchestrate compute resources previously monopolized by platform companies for user tracking and recommendation algorithms. 

The competitive advantage [or **DISADVANTAGE for those who choose not to bother**] lies in mastering these automation tools ... it comes down to ***understanding*** [GitHub Actions workflows](https://docs.github.com/en/actions), debugging MCP server logs, monitoring [dashboards](https://grafana.com/docs/grafana/latest/), and troubleshooting agent communication failures. Just as we no longer work in organizations where secretaries type and distribute interoffice memos, the economic divide of the future will increasingly separate those who can configure [Kubernetes clusters](https://kubernetes.io/docs/home/) for agent deployment, interpret and troubleshoot [Datadog metrics](https://www.datadoghq.com/), and debug protocol handshakes FROM the less competent who are limited to surface-level ChatGPT interactions or just *googling* a topic even with AI mode. 

While maintaining our functional mdBook is sufficient for where we are at in the process of developing our PKM, mastering the underlying automation infrastructure—from [CI/CD pipelines](https://www.redhat.com/en/topics/devops/what-is-ci-cd) to distributed tracing—becomes essential for extracting trustworthy insights from tomorrow's exponentially growing data sources. The technical backbone combines two complementary protocols: [MCP](https://modelcontextprotocol.io/docs/getting-started/intro) connects agents to things like [PubMed databases](https://pubmed.ncbi.nlm.nih.gov/), or [Salesforce CRMs](https://www.salesforce.com/products/platform/overview/), while [A2A](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/) enables agent teams to divide complex tasks—one agent might analyze market trends while another evaluates technical feasibility, coordinating through A2A messaging.

Our PKM must be able to function as an automated research assistant across [specialized knowledge domains](https://github.com/AncientGuy/PKM/tree/main/src/2.Areas), each configured with:
- **Custom AGENTS.md files**: Define agent personas, capabilities, and constraints (e.g., "Financial Analyst Agent" with SEC filing expertise)
- **[MCP servers](https://modelcontextprotocol.io/docs/getting-started/intro)**: Gateway to [Snowflake data warehouses](https://www.snowflake.com/), [Jupyter notebooks](https://jupyter.org/), or [Tableau dashboards](https://www.tableau.com/)
- **[A2A coordination](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/)**: Enable agent collaboration—a legal research agent might request patent analysis from a technical agent

Different real-world examples come to mind: An aerospace engineer analyzing satellite constellation optimization would deploy agents that simultaneously pull from [STK orbital mechanics software](https://www.ansys.com/products/missions/ansys-stk), [NOAA space weather data](https://www.swpc.noaa.gov/), and [SpaceX launch manifests](https://www.spacex.com/launches/). Tasks requiring 20-person teams at Lockheed Martin become solo endeavors. *How is it possible to accomplish this?* Part of it comes down to the fact that the compute power we can afford in 5 years will be 10X more than what we now spend. But there's more to it than that -- **it's not just the compute, but the awareness of that compute is being or has been used for.** The compute power churning away in consumer devices—previously harvesting behavioral data for [Meta's ad targeting](https://www.facebook.com/business/ads) or [Amazon's recommendation engines](https://aws.amazon.com/personalize/)and myriad other consumer tracking application—can now process [inferences based on genomic sequences](https://www.ncbi.nlm.nih.gov/sra) or run [weather simulations of REAL value to people who work in weather, rather than fear-mongering climate hysteria](https://www.cesm.ucar.edu/). 

Critical success factors include:
- **Observability**: [OpenTelemetry instrumentation](https://opentelemetry.io/) to trace agent decisions
- **Reliability**: [Prometheus monitoring](https://prometheus.io/) for MCP server uptime
- **Debugging**: [Elasticsearch](https://www.elastic.co/elasticsearch/) for searching through agent conversation logs
- **Validation**: Automated testing pipelines using [pytest](https://docs.pytest.org/) or [Jest](https://jestjs.io/)

Though our mdBook serves current needs, the strategic imperative involves building robust automation infrastructure now—before agent-augmented analysis becomes table stakes for professional competence in any knowledge-intensive field. Specifically for these Modules, at this early point of Phase I of our 100-day process, now that we have mapped out where this PKM thing is going to take us, we want to delve into the simplest thing that we have currently working.  We want to examine the deployment of mdBook to GitHub Pages with a finer tooth comb, to [begin to understand how to use GitHub Actions workflows to automate tasks](https://docs.github.com/en/actions/how-tos/write-workflows) including the [GH Actions Extension for VS Code](https://github.com/github/vscode-github-actions) to help us more efficiently, rapidly [learn or re-learn somethings about YAML](https://learnxinyminutes.com/yaml/) which is a superset of [JSON](https://www.json.org/json-en.html). Eventually, we might even want to follow along with those [contributing](https://github.com/github/vscode-github-actions/blob/HEAD/CONTRIBUTING.md) to the [GitHub Actions VS Code Extension](https://github.com/orgs/github/projects/9557) or, since [GitHub Actions and automation of GitHub workflows](https://github.com/actions) is so critical, we might be interested in the larger [public roadmap of GitHub product development features](https://github.com/orgs/github/projects/4247).

At first we might just to explore the [.github/workflows/mdbook.yml](https://github.com/AncientGuy/PKM/blob/main/.github/workflows/mdbook.yml) YAML file with the [GH Actions Extension for VS Code](https://github.com/github/vscode-github-actions) ...  and ***READ*** lots of the [GitHub Actions documentation](https://docs.github.com/en/actions) ... but at some point, we will probably need to test our knowledge by ***breaking*** our mdbook.yml page in different ways and test it in different ways with [an online YAML linter](https://www.yamllint.com/) or study  to have a better inkling of understanding for potential future troubleshooting of issues that cause a breakage -- for example, we might notice certain things that could go awry in the future.

Changes that we do not control will happen -- it's likely that there be some change in GitHub mgmt's vision of what features GitHub should offer. Also, we might want to make "minor changes" to things that we control. For example, maybe we decide that the repository settings should be configured to deploy from a specific branch; many monorepos that have many different things going on will use a clearly-labeled branch named something like gh-pages. We typically run the 'mdbook serve --open' build command run locally to generate the static site in the book/book directory; the contents of this local output directory could be manually committed and pushed to separate gh-pages branch. *But even little changes are not often trivial or inconsequential ...we should expect that changes will happen, affecting things that nobody foresaw.*

Of course, changes are not necessarily bad -- but often necessary, massively-positive long overdue change produces unintended consequences, especially in things that are not used by many ... but the push for CHANGE happens, because that change is needed for a larger advance. For example, at this time GitHub is not able to support the GH Actions extension with remote repositories (including github.dev and vscode.dev), because enough people are not demanding this ... but failure to support remote repositories confines the use of GH Actions locally downloaded GitHub repositories that get synched with GitHub's central repositories. But we can expect updates on something like this in the future and when that happens, it's ***almost*** certain that some functionality that now just works will not work flawlessly when that major feature of working with remote repositories is added.

*We should expect that BIG changes in knowledge engineering technology will happen ... AND WITH GREATER FREQUENCY, ie because it will be expected that devs using agentic workflows will be able to overcome hurdles that in the past have ensured backward compatibility, ie as a result of AI-assisted IDEs we should expect that backward compatibility will be less of a concern.* ***We should foresee a day where it is expected that users of technology not be as helpless or unable to use AI-assistance, ie it's like expecting a trucker should be able to figure out how use a sophisticated navigation system that trucking company and all of its drivers must depend upon.*** 

This exercise provides the basis of a more complete understanding of the build artifacts and the deployment mechanism that will be automated in a later phase. We want to do more more than just know [how to build mdBook with Github Actions today](https://levelup.gitconnected.com/how-to-build-mdbook-with-github-actions-eb9899e55d7e) ... because GitHub is always changing, improving things -- making things safer, so our current knowledge might be deprecated by some future change... we have used the cookbook template to publish our mdBook to GitHub pages and, as we might expect, this approach has been fool-proofed enough by GH over the years that the GH template approach tends to just work ... ***for NOW!*** Instead of just being grateful that it works *somehow*, not asking any questions and moving on to other things we'll only superficially understand, we really NEED to spend a wee bit of time developing a much deeper understanding of the process and pains of automation, so we will start going over some suggested blog postings like [the Beginner's Guide To Python Automation Scripts (With Code)](https://zerotomastery.io/blog/python-automation-scripts-beginners-guide/) or [19 Super-Useful Python Scripts to Automate Your Daily Tasks](https://www.index.dev/blog/python-automation-scripts).

We will want to go back and spend some more time with [the DETAILS in mdBook Documentation](https://rust-lang.github.io/mdBook/) that we may have glanced over, especially in the realm of [renderers](https://rust-lang.github.io/mdBook/format/configuration/renderers.html) and [mdBook's approach to Continuous Integration](https://rust-lang.github.io/mdBook/continuous-integration.html) with something like GitHub, before [trying to create our first CI/CD Pipeline Using GitHub Actions](https://brandonkindred.medium.com/creating-your-first-ci-cd-pipeline-using-github-actions-81c668008582) or exploring [different ways that people have used GitHub Actions to deploy static sites](https://github.com/peaceiris/actions-gh-pages), specifically [step by step instructions for overcoming problems publishing an mdBook on gh-pages](https://github.com/rust-lang/mdBook/issues/1803).